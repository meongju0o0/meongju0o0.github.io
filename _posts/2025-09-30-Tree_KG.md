---
layout: single
title:  "Tree-KG: An Expandable Knowledge Graph Construction Framework for Knowledge-intensive Domains"
categories: PaperReview
tag: [deep_learning, paper_review, recommender_system]
toc: true
toc_sticky: true
author_profile: true
---

# Tree-KG: An Expandable Knowledge Graph Construction Framework for Knowledge-intensive Domains
## 논문 정보
- Songjie Niu, Kaisen Yang, Rui Zhao, Yichao Liu, Zonglin Li, Hongning Wang, Wenguang Chen
- ACL 2025: The 63rd Annual Meeting of the Association for Computational Linguistics
- Tsinghua University, Tsinghua Shenzhen International Graudate School

## 1. Introduction
- 과학 연구처럼 복잡하고 전문적인 분야의 지식을 체계적으로 정리하기 위해, **Tree-KG**라는 새로운 지식 그래프(Knowledge Graph, KG) 자동 구축 프레임워크를 제안
- 인간이 교과서 목차처럼 지식을 계층적으로 구성하는 방식에서 영감을 얻어 이를 자동화 하는 것

### 핵심 문제
- 과학, 의료, 법률 등 지식 집약적 분야에서는 방대한 데이터를 구조화하여 의사결정에 활용하는 것이 중요
- 지식 그래프(KG)가 좋은 해결책지만, 이 분야들은 내용이 너무 복잡하고 빠르게 변해서 KG를 만드는 데 많은 수작업과 노력이 필요

### 기존 연구읜 한계점
- 규칙 기반 시스템 (Rule-based systems)
    - 높은 정확도를 보여주나, 확장성과 일반화 능력이 부족, 새로운 상황에 취약
    - 반대로, DL 및 임베딩 기반은 확장성을 위한 별도의 모델 설계 혹은 유사도 연산을 통한 일반화 능력이 우수
- 지도 학습 모델 (Supervised learning methods)
    - 고품질의 데이터셋을 만드는 데 많은 비용과 노력이 필요
    - 학습 데이터에 대한 의존도가 높아 적응성이 떨어짐
- LLM 기반 방법
    - 자동화에 유용
    - 잘 정의된 지식 구조나 의미적 일관성이 부족 (hallucination이 그대로 반영된 KG를 구축)
    - 지식을 점진적으로 확장하는 자체 매커니즘이 없어 확장성에 한계가 존재

### 핵심 아이디어
- 인간이 교과서 목차처럼 지식을 계층적으로 구성하는 방식에서 영감을 얻음
- 실제 물리 교과서 데이터를 분석한 결과, 교과서상 가까운 위치에 있는 개념일수록 관계성이 강하다는 사실을 확인
- 이를 바탕으로 교과서의 구조를 적극적으로 활용하는 'Tree-KG' 프레임워크를 개발

- Figure 1은 교과서의 각 section에 등장하는 entity 간의 관계성 점수(strength score)를 시각화한 결과
- 자기 자신에 대한 관계성 점수가 가장 높게 나타나며
- 주변 section의 entity와의 관계성 점수가 그 다음으로 높게 나타나는
- 경향을 확인할 수 있음

<div align="center">
    <img src="/images/2025-09-30-Tree_KG/figure_1.png" alt="figure_1" width="500">
</div>

### 해결책: Tree-KG
- Tree-KG는 이러한 문제를 해결하기 위해 아래 두 단계 접근 방식을 사용
    1. **초기 구축 (뼈대 만들기)**
        - 먼저 교과서나 논문처럼 목차가 있는 구조화된 텍스트를 분석
        - 대형언어모델(LLMs)을 활용하여 이 목차 구조를 그대로 따라 트리(Tree) 형태의 기본 지식 그래프를 구축
        - 이는 마치 건물의 뼈대를 세우는 skeleton 구축 과정과 동일
    2. **반복적 확장 (살 붙이기)**
        - 기본 뼈대가 만들어지면, 미리 정의된 유연한 규칙(operator)을 통해 숨겨진 관계를 찾아내어 그래프를 점진적으로 확장
        - 예를 들어, 다른 챕터에 있지만 서로 관련 있는 개념들을 찾아 연결하는 식
        - 이를 통해 뼈대에 살을 붙여 풍성하고 완성도 높은 지식 그래프를 구축

### 주요 결과 및 장점
- **뛰어난 성능**
    - 실험 결과, Tree-KG는 기존 다른 방법들보다 월등한 성능을 보임
    - F1 점수가 2위 그룹보다 12~16% 더 높았음
- **높은 정보 추출 품질**
    - 소스 텍스트에서 정보를 정확하게 추출하는 능력이 뛰어남
    - 특정 데이터셋에서는 0.81이라는 높은 F1 점수를 기록
- **구조적 우수성**
    - 교과서 구조를 기반으로 하여 생성한 KG는 논리적으로 잘 정렬되어 있음
    - 특정 전문 분야의 지식을 효과적으로 표현
- **비용 효율성**
    - 더 적은 LLM 토큰(비용)을 사용하면서도 강력한 결과를 도출
    - 비용 효율적이고 자원 친화적인 구축이 가능

## 2. Related Work
### 1. 전통적 방식: 규칙 기반 및 지도 학습
- 규칙 기반 (Rule-based)
    - "A는 B의 수도이다"와 같은 문법적, 의미적 규칙을 사전에 정의하여 지식을 추출
    - 정확도는 높지만, 규칙을 만들기 어렵고 새로운 형태의 문장에는 적용하기 힘든 단점이 있음
- 지도 학습 (Supervised)
    - 정답이 표시된 대규모 데이터셋을 모델에 학습시켜 관계 추출 패턴을 익히게 만듦
    - 성능은 좋지만, 고품질의 학습 데이터를 만드는 데 많은 비용과 시간이 들고 확장성이 부족

### 2. LLM 기반 방식: 더 똑똑하고 유연한 접근
- 옽톨로지 기반 (Ontology-based)
    - 위키피디아처럼 이미 잘 구축된 지식 체계(온톨로지)를 기반으로
    - LLM을 활용해 새로운 텍스트에서 관계 후보를 생성하고 기존 지식에 연결
    - 기존 지식 체계의 품질에 성능이 크게 좌우됨
- 미세 조정 (Fine-Tuning)
    - 특정 데이터셋에 맞게 LLM을 추가로 학습시켜 관계 추출 성능을 극대화
    - 특정 환경에서는 높은 성능을 보이나, 학습하지 않은 새로운 데이터나 너무 많은 종류의 관계를 처리하는 데는 어려움이 존재
- 제로샷/퓨샷 학습 (Zero- & Few-shot Learning)
    - 별도의 학습 데이터 없이(제로샷) 또는 아주 적은 예시만으로(퓨샷) LLM에게 지시하여 지식을 추출
    - 데이터 준비 부담이 적지만, 때때로 개념을 정확히 구분하지 못하거나 계산 비용이 많이 들 수 있음
- 지식 집약적 도메인 특화
    - 수학(MathGraph)이나 교육처럼 특정 전문 분야에 맞춰 LLM을 전문가처럼 활용하는 시스템
    - 높은 정확도를 보이지만, 해당 분야에만 적용할 수 있어 범용성이 떨어짐

### 결론: 균형 잡힌 프레임워크의 필요성
- 결론적으로, 지식 집약적 분야를 위한 효과적인 KG 구축 프레임워크는 **자동화, 정확성, 적응성, 지식 통합 능력** 사이에서 균형을 맞추어야 함
- 이 글의 저자들이 제안하는 **Tree-KG**는 바로 이러한 균형을 목표로 하는 프레임워크

## 3. Methodology
### 1. 트리 형태의 계층적 그래프 (Tree-like Hierarchical Graph)
- Tree-KG가 생성하는 지식 그래프의 기본 구조
- 트리 자료구조와 같이 여러 계층으로 구성

- 정점 (Node, V): 그래프의 각 개념(엔티티)은 특정 계층($V_1, V_2, ..., V_k$)에 속함
    - 정점 집합 $V$는 $k$ 레이어로 분할됨 (The node set $V$ is partitioned into $k$ layers)
        - $V = V_1 \cup V_2 \cup ... \cup V_k$
    - 각 부분 집합 $V_i$는 특정 레이어와 대응 (where each subset $V_i$ corresponds to a specific layer)
        - $V_i \cup V_j = \emptyset$ for $\forall (i \neq j)$

- 간선 (Edge, E): 두 종류의 간선이 존재
    - 수직 간선 (Vertical Edges, $E_1$)
        - 서로 다른 계층의 노드를 연결
        - e.g., '물리학' 챕터 → '전자기학' 섹션
        - 위 간선들은 전체적으로 트리 구조를 형성
    - 수평 간선 (Horizontal Edges, $E_2$)
        - 동일한 계층 내의 정점을 연결
        - e.g., '전하' 개념 ↔ '전자' 개념

- 수직 간선 (Verical Edges, $E_1$)
    - **서로 다른 계층** 간의 연결을 구성
    - 인접한 계층 간에만 연결을 구성 (the edge can only connect adjacent layers)
        - For each edge $(u, v) \in E_1$, if $u \in V_i$ and $v \in V_j$, then $\vert i-j \vert = 1$

- 수평 간선 (Horizontal Edges, $E_2$)
    - **서로 같은 계층** 간의 연결을 구성
    - For each edge $(u, v) \in E_2$, if $u, v \in V_i$, tehn the edge connects nodes within the same layer $V_i$

### 2. 지식 그래프 스키마 (KG Schema)
- 지식 그래프를 구성하는 정점과 간선의 구체적인 설계 명세

- 정점 (Node)의 속성
    - 이름 (name): 정점의 고유한 이름 (e.g., 'Electric Charge')
    - 설명 (description): 텍스트에서 추출한 개념 설명
    - 관계 (relations): 해당 정점에 연결된 간선 정보
- 간선 (Edge)의 종류
    - 수직 간선
        - `has_subsection` (상위섹션-하위섹션), `has_entity` (섹션-개념), `has_subordinate` (핵심개념-비핵심개념) 등 계층/소속 관계를 표현
    - 수평 간선
        - LLM이 예측한 구체적인 관계(e.g., 'obey', 'has')와 일반적인 카테고리(`section_related`, `entity_related`)를 결합하의 의미 관계를 표현

### 3. 아다믹-아다 점수 (Adamic-Adar Score)
- 두 정점 $(u, v)$가 얼마나 연결될 가능성이 높인지를 공통 이웃(common neighbors)을 기반으로 계산

- 핵심 아이디어
    - 단순히 공통 이웃이 많다고 중요한 것이 아님
    - **영향력 있는** 공통 이웃을 공유하는 것이 더 중요
    - 여기서 **영향력**은 해당 이웃이 얼마나 적은 수의 다른 노드와 연결되어 있는지(희소성)으로 판단
- 수식
    - $AA(u,v)=\sum_{w \in N(u) \cap N(v)}{\frac{1}{log(\vert N(w) \vert)}}$
        - $N(u)$: Set of neighbors of node $u$ (i.e., 정점 $u$의 이웃 정점)
        - $N(v)$: Set of neighbors of node $v$ (i.e., 정점 $v$의 이웃 정점)
        - $\vert N(w) \vert$: Degree of node $w$ (i.e., 정점 $w$의 차수)
        - AA 점수를 계산할 시에는 그래프 전체를 무방향으로 간주

### 4. 공통 조상 정점 수 (Number of Common Ancestors)
- 두 정점$(u, v)$가 계층적으로 얼마나 가까운지를 측정하는 지표
- 계산 방식
    - **Lineage Path**
        - 
    - **Common Ancestors**
        - 
    - 각 정점에서 수직 간선을 다라 최상위 계층(루트)까지 올라가는 모든 경로를 찾음
    - 두 정점의 경로 간에 겹치는 조상 정점의 최대 개수를 계산
    - 공통 조상이 많을 수록 두 정점은 구조적으로 더 가깝다고 간주